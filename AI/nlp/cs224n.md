## 二、[GloVe及词向量的训练与评估](http://blog.showmeai.tech/cs224n/note02-GloVe-Training-and-Evaluation)

### 1.Glove

https://www.biaodianfu.com/glove.html#GloVe%E7%AE%80%E4%BB%8B

GloVe 由一个加权最小二乘模型组成，基于全局word-word 共现计数进行训练，从而有效地利用全局统计数据。

**如何从统计信息产生意义，以及产生的单词向量如何表示这些意义？**

Glove就是解决这个问题。

X表示word-word共现矩阵，其中$X_{ij}$表示词j 出现在词 i 的上下文的次数。令$X_i=\sum_{k}X_{ik}$为任意词 k 出现在词 i 的上下文的次数。

$P_{ij}=P(w_j|w_i)=\frac{X_{ij}}{X_i}$是词 j 出现在词 i 的上下文的概率。
$$
F(w_i,w_j,w_k)=\frac{P_{ik}}{P_{jk}}
$$
表示对于上下文词 k ，跟 i 的相似度 比上 跟 j 的相似度。



### 4.suggest reading

#### Improving DIstributianal Similarity with Lessons learned from Word Embeddings

**实际应用中如何获得更好的词向量**

虽然模型和优化的目标函数是主要因素，但是其他因素也会影响结果。

超参数优化：负采样的样本个数，平滑的负采样分布，动态大小的上下文窗口

一个经验法则：始终使用上下文分布平滑（cds=0.75）来修改PMI，并且适用于PPMI，SVD，和SGNS，不断提高性能。

#### Evaluation methods for unsupervised word embeddings

**无监督字嵌入的评估方法**

