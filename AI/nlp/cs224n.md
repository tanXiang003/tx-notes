## 二、[GloVe及词向量的训练与评估](http://blog.showmeai.tech/cs224n/note02-GloVe-Training-and-Evaluation)

### 1.Glove

https://www.biaodianfu.com/glove.html#GloVe%E7%AE%80%E4%BB%8B

GloVe 由一个加权最小二乘模型组成，基于全局word-word 共现计数进行训练，从而有效地利用全局统计数据。

**如何从统计信息产生意义，以及产生的单词向量如何表示这些意义？**

Glove就是解决这个问题。

X表示word-word共现矩阵，其中$X_{ij}$表示词j 出现在词 i 的上下文的次数。令$X_i=\sum_{k}X_{ik}$为任意词 k 出现在词 i 的上下文的次数。

$P_{ij}=P(w_j|w_i)=\frac{X_{ij}}{X_i}$是词 j 出现在词 i 的上下文的概率。
$$
F(w_i,w_j,w_k)=\frac{P_{ik}}{P_{jk}}
$$
表示对于上下文词 k ，跟 i 的相似度 比上 跟 j 的相似度。



### 4.suggest reading

#### Improving DIstributianal Similarity with Lessons learned from Word Embeddings

**实际应用中如何获得更好的词向量**

虽然模型和优化的目标函数是主要因素，但是其他因素也会影响结果。

超参数优化：负采样的样本个数，平滑的负采样分布，动态大小的上下文窗口

一个经验法则：始终使用上下文分布平滑（cds=0.75）来修改PMI，并且适用于PPMI，SVD，和SGNS，不断提高性能。

#### Evaluation methods for unsupervised word embeddings

**无监督字嵌入的评估方法**

## 五、语言模型、RNN、GRU、LSTM2

### 1.Language Models

#### 1.1 Introduction

语言模型计算特定序列中多个单词出现的概率。
$$
P(w_1,...,w_m)=\prod_{i=1}^{i=m}P(w_i|w_1,...,w_{i-1})\approx\prod_{i=1}^{i=m}P(w_i|w_{i-n},...,w_{i-1})
$$
上式在语音识别和机器翻译中对判断一个词序列是否是一个输入句子的准确翻译i起到了重要的作用。

#### 1.2 n-gram Language Models

![image-20221018194038057](cs224n.assets/image-20221018194038057.png)

一般要加上拉普拉斯平滑操作，防止分子或者分母为0。   

一般 n<=5，n太大维度灾难。

#### 1.3 Window-based Neural Language Model (NNLM)

![image-20221018194452877](cs224n.assets/image-20221018194452877.png)

### 2.RNN（Recurrent Neural Networks）

传统的翻译模型只能以有限窗口大小的前n个单词作为条件进行语言模型建模，而循环神经网络能以语料库所有前面的单词作为条件进行语言模型建模。

![image-20221018200835579](cs224n.assets/image-20221018200835579.png)

![image-20221018195629796](cs224n.assets/image-20221018195629796.png)

其中，每一个时间步的参数使用相同的权重$W^{(hh)} 和 W^{(hx)}$。这样模型需要学习的参数变少了。

#### 2.1 RNN Loss and Perplexity

RNN中常用的损失函数是交叉熵误差。

![image-20221018195914059](cs224n.assets/image-20221018195914059.png)

在大小为T的语料库上的交叉熵误差的计算如下：

![image-20221018200039729](cs224n.assets/image-20221018200039729.png)

#### 2.2 Advantages ,Disadvantages, and Applications of RNNs

RNN优点：

1. 它可以处理任意长度的序列
2. 对更长的输入序列不会增加模型的参数大小
3. 对时间步t的理论可以利用前面很多时间步的信息
4. 权重不变，因此在处理输入时具有对称性。

缺点：

1. 计算速度很慢，依赖上一个时间步，所以不能优化

2. 实际中因为梯度消失和梯度爆炸，很难用到前面时间步的信息。

运行一层RNN所需的内存量与语料库里的单词数成正比。对于一个k个单词的句子在内存中会占用k个词向量的存储空间，同时，W和b也有可能很大，对于有1000个循环层的RNN，矩阵W的大小为1000 x 1000.

RNN可以应用到标注任务（词性标注、命名实体识别），句子分类（情感分类），编码模块（问答、机器翻译）。

#### 2.3 Vanishing Gradient & Gradient Explosion Problems 

梯度消失使得一个长文本在反向传播的过程中后面的梯度难以传到前面，所以在实际使用中无法预测长文本后面的单词。

#### 2.4 Solution to the Exploding & Vanishing Gradients

Mikolov 提出解决梯度爆炸的解决方法：每当梯度大于一个阈值的时候，将其截断为一个很小的值，![image-20221018203513105](cs224n.assets/image-20221018203513105.png)

梯度消失的解决方法：1. 不去随机初始化$W^{(hh)}$，而是初始化为单位矩阵。

2. 使用RELU单元代替sigmoid函数。RELU的导数是0或者1，在反向传播的过程中不会出现梯度消失问题。

#### 2.5 Deep Bidirectional RNN

双向RNN：不仅从前往后预测，还从后往前预测。

![image-20221018204200543](cs224n.assets/image-20221018204200543.png)

#### 2.6 Application:RNN Translation Model

采用RNN替代传统翻译模型。            下图将德语短语 Echt dicke Kiste翻译为Awesome sauce。

![image-20221019115134718](cs224n.assets/image-20221019115134718.png)

前三个时间步的隐藏层编码 德语单词 成为一些语言的单词特征（$h_3$），后面两个时间步解码 $h_3$为英语单词输出。

下式分别展示了编码阶段（23）和解码阶段（24 、25）。

![image-20221019115311071](cs224n.assets/image-20221019115311071.png)

一般使用交叉熵损失函数。

![image-20221019115904170](cs224n.assets/image-20221019115904170.png)

一些扩展可以改善这个模型的准确性。

1. 在编码和解码阶段使用不同的权重，可以将两个阶段解耦，使两个模块进行更准确的预测。这意味着（23）和（24）式有不同的 $W^{(hh)}$。

2. 使用三个不同的输入计算解码器中的每个隐藏状态。

   - 前一个隐藏状态$h_{t-1}$

   - 编码阶段的最后一个隐藏层c

   - 前一个预测的输出单词$y_{t-1}$

     从而将（24）式中的$\phi$函数变为  $h(t)=\phi(h_{t-1},c,y_{t-1})$

3. 使用多个RNN层来训练深度循环神经网络，需要大规模的语料库。
4. 双向编码器。
5. 反转输入句子的顺序有助于降低输出短语的错误。

## 3.Gated Recurrent Units

门控制单元让RNN具有更多的持久性内存，从而更容易捕获长距离信息。11

![image-20221019142824050](cs224n.assets/image-20221019142824050.png)



![image-20221019144333461](cs224n.assets/image-20221019144333461.png)

四个基本阶段：

- New memory genertion:一个新的记忆$\widetilde{h_t}$是由一个新的输入单词$x_t$和过去的隐藏状态$h_{t-1}$共同计算所得。
- Reset Gate : 复位信号$r_t$是负责确定$h_{t-1}$对总结$\widetilde{h_{t}}$的重要程度。如果确定$\widetilde{h_{t}}$与新的记忆无关，则复位门能够完全消除过去的隐藏状态。
- Update Gate:更新信号$z_t$负责确定有多少$h_{t-1}$可以向前传递到下一个状态。
- Hidden state:利用更新门的建议，使用过去的隐藏输入$h_{t-1}$和新生成的记忆$\widetilde{h_{t}}$生成隐藏状态$h_t$。

需要通过反向传播训练参数 $W,U,W^{(r)},U^{(r)},W^{(z)},U^{(z)}$。

### 4. Long-Short-Term-Memories

http://colah.github.io/posts/2015-08-Understanding-LSTMs/

![image-20221019150014108](cs224n.assets/image-20221019150014108.png)

1. New Memory generation:这个阶段类似于GRU生成新的记忆的阶段。
2. Input Gate:输入门使用输入词和过去的隐藏状态来决定输入值是否值得保存，从而用来进入新内存。
3. Forget Gate: 与输入门类似，不过不是评估输入词的有用性，而是评估过去记忆对当前记忆的计算有用。
4. FInal Memory generation: 首先根据忘记门$f_t$的判断，相应的忘记过去的记忆$c_{t-1}$。类似的，根据输入门$i_t$的判断，输入新的记忆$\widetilde{c_t}$。将结果相加得到最终的记忆$c_t$。
5. Output/Exposure Gate: 从隐藏状态中分离最终的记忆。
