## 1. 连续词袋模型

### 1.1 one-word context（上下文一个词）

从词袋模型的最简单版本开始。我们假设每个上下文只考虑一个单词，这意味着给定模型一个上下文词去预测一个目标词，就像bigram模型。

图1显示了简化了上下文定义下的网络模型。规定，词汇量大小为V，隐含层大小为N。相邻层的单元是全连接的，输入是一个one-hot 向量，这意味着给的一个上下文单词，{${x_1,x_2 ... x_v}$}中只有一个1。

![image-20221010164521603](word2vec Parameter Learning Explained.assets/image-20221010164521603.png)

输入层和输出层之间的权重可以表示成一个V×N的矩阵W。W的每一行是一个N维的向量$V_W$。

通常，W的行$i$ 是$v_w^T$。给定一个上下文单词，设$x_k = 1$，则 
$$
h = W^Tx = W^T_{(k,.)}:=v_{w_I}^T               \quad (1)
$$
也就是将 W 的第 k 行复制到 h，$v_{w_I}$是输入单词$w_I$的向量表示。这暗示了隐含层的激活函数就是一个简单的线性函数。

从隐含层到输出层，有一个不同的权重矩阵，$W^‘={w_{ij}^‘}$，是一个$N × V$的矩阵。使用这些权重，我们可以为词汇表中每一个单词计算一个分数 $u_j$，
$$
u_j = v_{w_j}^{'T}h \quad (2)
$$
其中，$v_{w_j}^{'}$是$W^{'}$的第 j 列，即一个 $ N * 1$的矩阵。

然后可以使用softmax，一个对数线性分类模型，去得到词的后验分布，是一个多项分布。
$$
p(w_j|w_I) = y_j = \frac{exp(u_j)}{\sum_{j^{'}=1}^{V}exp(u_{j^{'}})} \quad(3)
$$
其中，$y_j$是输出层单元 j 的输出。把$(1) 和(2)$带入 $(3)$得到
$$
p(w_j|w_I) = \frac{exp(v_{w_j}^{'T}v_{w_I})}{\sum_{j^{'}=1}^{V}exp(v_{w_{j^{'}}}^{'}v_{w_I})}\quad(4)
$$
注意，$v_w$和$v_w^{'}$是单词$w$的两种表示形式。$v_w$来自W的行向量，是输入--隐含层的矩阵。而$v_w^{'}$来自$W^{'}$的列，是隐含层--输出层的矩阵。在随后的分析中，称$v_w$为“输出向量”，而$v_w^{'}$为“输出向量”，它们都是单词 $w$的向量。

**更新隐含层---》输出层权重的方程**

训练目标是最大化 （4）式，即最大化给定上下文单词 $w_I$下输出一个正确的输出 $w_O$的条件概率。
$$
maxp(w_O|w_I)=maxy_j*\quad(5)\\=maxlogy_j*\quad(6)\\=u_j*-log\sum_{j^{'}=1}^{V}exp(u_{j^{'}}):=-E\quad(7)
$$
其中，$E=-logp(w_O|w_I)$是我们的代价函数（我们想要最小化E），$j*$是输出层实际输出的索引。注意：这个损失函数可以理解为两个概率分布之间的交叉熵测量的特殊情况。

下面推导隐含层和输出层之间的权重更新公式。
$$
\frac{\partial E}{\partial u_j}=y_j-t_j:=e_j\quad(8)
$$
$t_j=1$只在第 j 单元是实际输出单词，否则为0. 注意，这个导数就是输出层的预测误差ej。

接下来对$w_{ij}^{'}$求导获得隐含层---》输出层权重的梯度。
$$
\frac{\partial E}{\partial w_{ij}^{'}}=\frac{\partial E}{\partial u_j}*\frac{\partial u_j}{\partial w_{ij}^{'}}=e_j*h_i\quad(9)
$$
因此，使用随机梯度下降，我们得到隐含层--》输出层的权重更新方程为
$$
w_{ij}^{'(new)}=w_{ij}^{'(old)}-\eta*e_j*h_j\quad(10)
$$
或者
$$
v_{w_j}^{'(new)}=v_{w_j}^{'(old)}-\eta*e_j*h\quad for j=1,2,...,V.\quad(11)
$$
其中，$\eta$是学习率，$e_j = y_j-t_j$，$h_i$是隐含层的第 i 个单元；$v_{w_j}^{'}$是输出向量$w_j$。

注意，这个更新方程意味着我们必须遍查词汇表中每一个可能的单词，检查其输出概率$y_j$，并与它的预期输出$t_j$进行比较(要么0，要么1)。如果 $y_j>t_j$，就减少隐含向量h的一部分，让$v_{w_j}^{'}$距离$v_{w_I}$更远；如果 $y_j<t_j$，就增加h的一部分，让$v_{w_j}^{'}$距离$v_{w_I}$更近；如果yj非常接近tj，那么根据更新方程，权值变化很小。

**更新输入--》隐含层的权重的方程**

用E对隐含层的输出求偏导
$$
\frac{\partial E}{\partial h_i}=\sum_{j=1}^{V}\frac{\partial E}{\partial u_j}*\frac{\partial u_j}{\partial h_i}=\sum_{j=1}^{V}e_j*w_{ij}^{'}:=EH_i\quad(12)
$$
其中，$h_i$是隐含层的第 i 个单元的输出；$u_j$在（2）中被定义，输出层的第j个单元的净输入；

$e_j=y_j-t_j$是输出层的第j个单词的预测误差。EH，一个N维的向量，是词汇表中所有单词的输出向量之和，并以他们的预测误差加权。

接下来要对W求导。展开（1）式可得
$$
h_i=\sum_{k=1}^{V}x_k*w_{ki}\quad(13)
$$
现在可以对W的每一个元素求导，
$$
\frac{\partial E}{\partial w_{ki}}=\frac{\partial E}{\partial h_i}*\frac{\partial h_i}{\partial w_{ki}}=EH_i*x_k\quad(14)
$$
这等价于x和EH的张量积，
$$
\frac{\partial E}{\partial W}=x \otimes EH=xEH^T\quad(15)
$$
从而得到一个 $V * N$的矩阵，因为x只有一个分量是非0 的，$\frac{\partial E}{\partial W}$只有一行是非0 的，并且那一行的值为$EH^T$，一个N维的向量。我们得到W的更新公式为，
$$
v_{w_I}^{new}=v_{w_I}^{old}-\eta EH^T\quad(16)
$$
其中，$v_{w_I}$是W的一行，是唯一上下文单词的“输入向量”，并且是W中唯一导数非零的行。在这个迭代之后，W的所有其他行将保持不变，因为它们的导数为零。

直观地说，由于向量EH是词汇表中所有单词的输出向量之和，其预测误差$e_j=y_j-t_j$，因此我们可以将 (16) 理解为将词汇表中每个输出向量的一部分加到上下文单词的输入向量上。如果在输出层，一个词$w_j$作为输出词的概率被高估($y_j > t_j$)，那么上下文词$w_I$的输入向量将趋向于远离$w_j$的输出向量;相反，如果$w_j$是输出字的概率被低估($y_j < t_j$)，则输入向量$w_I$将趋向于向输出向量$w_j$靠拢;

### 1.2 多词上下文

图2显示了具有多词上下文的CBOW模型。在计算隐层输出时，CBOW模型没有直接复制输入上下文词的输入向量，而是取输入上下文词的向量的平均值，并且使用输入层--》隐含层权重矩阵和平均向量的乘积作为输出。
$$
h = \frac{1}{C}W^T(x_1,x_2,...,x_C) = \frac{1}{C}(v_{w_1}+v_{w_2}+...+v_{w_C})^T
$$
C是上下文中的单词数，w1, · · · , wC是上下文中的单词数，$v_w$是单词w的输入向量。

损失函数为：









