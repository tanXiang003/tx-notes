# 向量空间中单词表示的有效估计

**摘要**

我们提出了两种新颖的模型架构，⽤于从⾮常⼤的数据集中计算单词的连续向量表⽰。这些表⽰的质量在 单词相似度任务中进行测量，并将结果与以前基于不同类型神经⽹络的最佳性能技术进行⽐较。我们观察 到以更低的计算成本⼤幅提⾼了准确性，即从 16 亿个单词数据集中学习⾼质量的单词向量只需不到⼀天 的时间。此外，我们表明这些向量在我们的测试集上提供了最先进的性能，⽤于测量句法和语义词的相似性。

## 1.介绍

许多当前的 NLP 系统和技术将单词视为原⼦单元 - 单词之间没有相似性的概念，因为它们表⽰为词汇表中的索引。这种选择有⼏ 个很好的理由 - 简单性、稳健性以及观察到在⼤量数据上训练的简单模型优于在较少数据上训练的复杂系统。⼀个例⼦是⽤于统 计语⾔建模的流行 N-gram 模型 - 今天，可以在⼏乎所有可⽤数据（数万亿个单词 [3]）上训练 N-gram。

然⽽，简单的技术在许多任务中都处于其极限。例如，⽤于⾃动语⾳识别的相关域内数据量是有限的性能通常由⾼质量转录语⾳数据的⼤⼩（通常只有数百万字）决定。在机器翻译中，许多语⾔的现有语料库只包含⼏⼗亿或更少的单词。因此，在某些情况下，简单地扩⼤基本技术不会带来任何重⼤进展，我们必须专注于更先进的技术。

随着近年来机器学习技术的进步，可以在更⼤的数据集上训练更复杂的模型，并且它们通常优于简单模型。可能最成功的概念是使 ⽤词的分布式表⽰[10]。例如，基于神经⽹络的语⾔模型明显优于 N-gram 模型 [1, 27, 17]。

### 1.1论文目标

本⽂的主要⽬标是介绍可⽤于从包含数⼗亿单词和词汇表中数百万单词的庞⼤数据集中学习⾼质量单词向量的技术。据我们所知， 之前提出的架构都没有成功地训练过更多超过⼏亿个单词，单词向量的适度维度在 50 到 100 之间。

我们使⽤最近提出的技术来测量结果向量表⽰的质量，期望相似的词不仅趋于彼此接近，⽽且词可以具有多个相似度[20]。早先在屈折语⾔的上下⽂中已经观察到这⼀点例如，名词可以有多个词尾，如果我们在原始向量空间的⼦空间中搜索相似词，就有可能找到具有相似词尾的词 [13 , 14】

有点令⼈惊讶的是，⼈们发现单词表⽰的相似性超出了简单的句法规则。使⽤词偏移技术，其中对词向量执行简单的代数运算，例如， 向量（“King”）-向量（“Man”）+向量（“Woman”）导致向量为最接近单词 Queen [20] 的向量表⽰。

在本⽂中，我们尝试通过开发新的模型架构来最⼤限度地提⾼这些向量运算的准确性，以保持单词之间的线性规律。我们设计了⼀ 个新的综合测试集来测量句法和语义规则1 ，并表明可以以⾼精度学习许多这样的规则。此外，我们讨论了训练时间和准确性如何取 决于词向量的维度和训练数据的数量。

### 1.2 前期工作

将单词表⽰为连续向量具有悠久的历史 [10, 26, 8]。在 [1] 中提出了⼀种⾮常流行的⽤于估计神经⽹络语⾔模型 (NNLM) 的模型 架构，其中使⽤具有线性投影层和⾮线性隐藏层的前馈神经⽹络来联合学习词向量表⽰和统计语⾔模型。这项⼯作已被许多其他⼈ 效仿

[13, 14] 中提出了另⼀个有趣的 NNLM 架构，其中⾸先使⽤具有单个隐藏层的神经⽹络来学习词向量。然后使⽤词向量来训练 NNLM。因此，即使没有构建完整的 NNLM，也可以学习词向量。在这项⼯作中，我们直接扩展了这个架构，并只关注使⽤简单模型学 习词向量的第⼀步。

后来表明，词向量可⽤于显着改进和简化许多 NLP 应⽤程序 [4, 5, 29]。词向量本⾝的估计是使⽤不同的模型架构进行的，并在各种 语料库上进行训练 [4, 29, 23, 19, 9]，并且⼀些得到的词向量可⽤于未来的研究和⽐较2 。然⽽，据我们所知，这些架构的训练计算 成本明显⾼于 [13] 中提出的架构，除了使⽤对⻆权重矩阵的某些版本的对数双线性模型 [23]。

## 2. 模型架构

提出了许多不同类型的模型来估计单词的连续表⽰，包括众所周知的潜在语义分析（LSA）和潜在狄利克雷分配（LDA）。

在本⽂中，我们专注于神经⽹络学习的单词的分布式表⽰，因为之前已经证明它们在保持单词之间的线性规律性⽅⾯的表现明显优 于 LSA [20, 31]；此外，LDA 在⼤型数据集上的计算成本⾮常⾼。

与 [18] 类似，为了⽐较不同的模型架构，我们⾸先将模型的计算复杂度定义为完全训练模型需要访问的参数数量。接下来，我们将 尝试最⼤化准确性，同时最⼩化计算复杂度。

对于以下所有模型，训练复杂度为 
$$
O = E × T × Q,
$$
其中 E 是训练 epoch （时间步）的数量，T 是训练集中的单词数，Q 为每个模型架构进⼀步定义。常⻅的选择是 E = 3 － 50 和 T ⾼达 10 亿。所有模型都使⽤随机梯度下降和反向传播[26]进行训练。

### 2.1 前馈神经网络语言模型（NNLM）

概率前馈神经⽹络语⾔模型已在 [1] 中提出。它由输⼊层、投影层、隐藏层和输出层组成。在输⼊层，N 个先前的单词 使⽤ 1-of-V 编码进行编码，其中 V 是词汇表的⼤⼩。然后使⽤共享投影矩阵将输⼊层投影到维度为 N × D 的投影 层 P。由于在任何给定时间只有 N 个输⼊处于活动状态，因此投影层的组合是⼀种相对便宜的操作。

NNLM 架构对于投影和隐藏层之间的计算变得复杂，因为投影层中的值是密集的。对于 N = 10 的常⻅选择，投影层 (P) 的⼤⼩可能为 500 到 2000，⽽隐藏层⼤⼩ H 通常为 500 到 1000 个单位。此外，隐藏层⽤于计算词汇表中所有 单词的概率分布，从⽽产⽣⼀个维数为 V 的输出层。因此，每个训练⽰例的计算复杂度为
$$
Q = N × D + N × D × H + H × V，
$$
其中主导项是 H × V 。但是，为了避免这种情况，提出了⼏种实际的解决⽅案；要么使⽤ softmax [25, 23, 18] 的分 层版本，要么通过使⽤在训练期间未归⼀化的模型来完全避免归⼀化模型 [4, 9]。使⽤词汇表的⼆叉树表⽰，需要评 估的输出单元的数量可以下降到log2(V ) 左右。因此，⼤部分复杂性是由 N × D × H 项引起的。

在我们的模型中，我们使⽤分层 softmax，其中词汇表表⽰为 Huffman ⼆叉树。这遵循了先前的观察，即单词的频率 对于在神经⽹络语⾔模型中获得类很有效 [16]。霍夫曼树将短⼆进制代码分配给频繁词，这进⼀步减少了需要评估 的输出单元的数量：虽然平衡⼆叉树需要评估log2(V ) 输出，但基于霍夫曼树的分层 softmax 只需要⼤约log2 （⼀ 元困惑度（V））。例如，当词汇量为 100 万个单词时，这会导致评估速度提⾼⼤约两倍。虽然这对于神经⽹络 LM 来 说并不是⾄关重要的加速，因为计算瓶颈在 N×D×H 项中，但我们稍后将提出没有隐藏层的架构，因此严重依赖于 softmax 归⼀化的效率。

### 2.2 循环神经网络语言模型（RNNLM）

已经提出了基于循环神经⽹络的语⾔模型来克服前馈 NNLM 的某些限制，例如需要指定上下⽂长度（模型 N 的阶 数），并且因为理论上 RNN 可以⽐浅层神经⽹络有效地表⽰更复杂的模式⽹络 [15, 2]。 RNN模型没有投影层；只 有输⼊层、隐藏层和输出层。这种模型的特别之处在于使⽤延时连接将隐藏层连接到⾃⾝的循环矩阵。这允许循环模 型形成某种短期记忆，因为过去的信息可以由隐藏层状态表⽰，该隐藏层状态根据当前输⼊和前⼀个时间步的隐藏 层状态进行更新。

RNN 模型的每个训练样例的复杂度为
$$
Q = H × H + H × V，
$$
其中单词表⽰ D 与隐藏层 H 具有相同的维度。同样，通过使⽤分层 softmax，可以将 H × V 项有效地简化为 H × log2(V )。⼤部分复杂度来⾃ H × H。

### 2.3 神经网络的并行训练

为了在庞⼤的数据集上训练模型，我们在⼀个名为 DistBelief [6] 的⼤规模分布式框架之上实现了⼏个模型，包括前馈 NNLM 和本⽂提出的新模型。该框架允许我们并行运行同⼀模型的多个副本，每个副本通过保留所有参数的集中式服务 器同步其梯度更新。对于这种并行训练，我们使⽤⼩批量异步梯度下降和称为 Adagrad [7] 的⾃适应学习率过程。在此框 架下，通常使⽤⼀百个或更多模型副本，每个副本在数据中⼼的不同机器上使⽤许多 CPU 内核。

## 3. 新的对数线性模型（线性模型+ softmax）

在本节中，我们提出了两种新的模型架构，⽤于学习单词的分布式表⽰，以尽量减少计算复杂度。上⼀节的主要观察是，⼤部 分复杂性是由模型中的⾮线性隐藏层引起的。虽然这就是神经⽹络如此吸引⼈的原因，但我们决定探索更简单的模型，这 些模型可能⽆法像神经⽹络那样精确地表⽰数据，但可以有效地在更多数据上进行训练。

新架构直接遵循我们早期⼯作 [13, 14] 中提出的架构，其中发现神经⽹络语⾔模型可以通过两个步骤成功训练：⾸先，使 ⽤简单模型学习连续词向量，然后使⽤ N- gram NNLM 在这些分布式单词表⽰之上进行训练。虽然后来有⼤量⼯作集中 在学习词向量上，但我们认为 [13] 中提出的⽅法是最简单的⽅法。

### 3.1连续词袋模型

第⼀个提出的架构类似于前馈 NNLM，其中去除了⾮线性隐藏层，所有单词共享投影层（不仅仅是投影矩阵）；因此，所有 单词都被投影到相同的位置（它们的向量被平均）。我们称这种架构为词袋模型，因为历史中的词顺序不会影响投影。

此外，我们还使⽤来⾃未来的词语；我们在下⼀节介绍的任务中获得了最佳性能，⽅法是构建⼀个对数线性分类器，输⼊有 四个未来词和四个历史词，其中训练标准是正确分类当前（中间）词。

训练复杂度为
$$
Q = N × D + D × log2(V )。 (4)
$$
我们将此模型进⼀步表⽰为 CBOW，与标准的词袋模型不同，它使⽤上下⽂的连续分布式表⽰。模型架构如图 1 所⽰。请注 意，输⼊层和投影层之间的权重矩阵以与 NNLM 中相同的⽅式为所有单词位置共享。

​	